---
# multilingual page pair id, this must pair with translations of this page. (This name must be unique)
lng_pair: 4_SVM
title: "Desmistificando as Máquinas de Vetores de Suporte (SVM) para Classificação."

# post specific
# if not specified, .name will be used from _data/owner/[language].yml
#author: "Deborah Cholodoysky Barbedo Pereira"
# multiple category is not supported
category: Aprendizado de Máquina
# multiple tag entries are possible
tags: [SVM, Classificação, VC Dimension, Hiperplanos, Vetores de Suporte, Margens Flexíveis, Truque do Kernel, Classificação Multiclasse, Aprendizado de Máquina]
# thumbnail image for post
img: ":WoE_IV_Python_calculo.jpg"
# disable comments on this page
#comments_disable: true

# publish date
date: 2023-07-01 19:57:53 +0900

# seo
# if not specified, date will be used.
#meta_modify_date: 2021-08-10 11:32:53 +0900
# check the meta_common_description in _data/owner/[language].yml
meta_description: "Neste post, explore o funcionamento das Máquinas de Vetores de Suporte (SVM) para classificação. Descubra conceitos como VC Dimension, hiperplanos, vetores de suporte e o truque do kernel. Uma introdução acessível ao SVM para iniciantes no campo do Aprendizado de Máquina."
# optional
# please use the "image_viewer_on" below to enable image viewer for individual pages or posts (_posts/ or [language]/_posts folders).
# image viewer can be enabled or disabled for all posts using the "image_viewer_posts: true" setting in _data/conf/main.yml.
image_viewer_on: true
# please use the "image_lazy_loader_on" below to enable image lazy loader for individual pages or posts (_posts/ or [language]/_posts folders).
# image lazy loader can be enabled or disabled for all posts using the "image_lazy_loader_posts: true" setting in _data/conf/main.yml.
image_lazy_loader_on: true
# exclude from on site search
#on_site_search_exclude: true
# exclude from search engines
#search_engine_exclude: true
# to disable this page, simply set published: false or delete this file
#published: false
---

<!-- outline-start -->

Uma exploração inicial das SVM, Dimensão VC, Hiperplanos, Vetores de Suporte, Margens Flexíveis, Truque do Kernel e Classificação Multiclasse.

<!-- outline-end -->

A máquina de vetores de suporte, em inglês Support Vector Machine (SVM),  é um algoritmo versátil usado tanto para regressão quanto para classificação. Ele é especialmente útil quando lidamos com variáveis aleatórias numéricas. Neste post, vamos nos concentrar em explicar como o SVM funciona para problemas de classificação.

O objetivo aqui é tornar a explicação o mais acessível possível, evitando o uso excessivo de demonstrações matemáticas e teoremas que possam parecer assustadores. Quero apresentar o SVM de forma superficial para que você possa entender a lógica por trás dele. Assim, você estará preparado para se aprofundar no assunto, já com uma base sólida sobre o propósito do algoritmo, ou mesmo para uma compreensão básica que permita aplicá-lo rapidamente.

# Dimensão Vapnik-Chervonenkis (VC):

A dimensão Vapnik-Chervonenkis (VC) é uma medida que avalia a capacidade de um conjunto de funções serem aprendidas por um algoritmo de classificação binária estatística. Em termos mais simples, a dimensão VC de um conjunto de funções é o número máximo de pontos que podem ser divididos de formas diferentes por essas funções. Essa medida foi proposta por Vapnik e Chervonenkis em 1971, e é extremamente útil para compreender a flexibilidade de um conjunto de funções na tarefa de classificação.


Exemplo: Dimensão VC de um conjunto de funções lineares para classificação de 2 grupos em um plano, é igual a 3.



![Dimensão Vapnik-Chervonenkis VC](:SVM\DimensaoVC.png){:data-align="center"}

Se tivéssemos que realizar uma classificação binária com 4 pontos, não seria possível encontrar uma função linear para todas as combinações possíveis.

![Exemplo classificação binária com 4 pontos](:SVM\Exemplo4.png){:data-align="center"}

# Hiperplano:

Um hiperplano é um plano de dimensão k-1 em um espaço de dimensão k. Por exemplo, em um espaço bidimensional, um hiperplano é uma reta, enquanto em um espaço tridimensional, é um plano.


![Um hiperplano é uma reta na dimensão 2. Um hiperplano é um plano na dimensão 3.](:SVM\Hiperplanodim2e3.png){:data-align="center"}

Conectando com o tópico anterior, a dimensão VC de um hiperplano em uma dimensão K é igual a K+1. Existem vários hiperplanos possíveis para classificar os dados:

![Hiperplanos](:SVM\Hiperplanos.png){:data-align="center"}

Mas o SVM visa encontrar o hiperplano que possui a maior margem de separação entre as duas classes:

![Hiperplano com maior margem](:SVM\Hiperplanocommaiormargem.png){:data-align="center"}

# Vetores de suporte:

Os vetores de suporte, também conhecidos como pontos críticos, são os pontos de dados que auxiliam na determinação da margem do classificador linear. Eles são fundamentais para a identificação do hiperplano ideal.

![Vetores de suporte](:SVM\Vetoresdesuporte.png){:data-align="center"}

# Margens suaves:

Para evitar o sobreajuste, é possível adotar margens suaves, permitindo que alguns pontos de dados fiquem dentro da margem de separação. Isso flexibiliza a classificação e torna o modelo mais robusto.

![Margens suaves](:SVM\Margenssuaves.png){:data-align="center"}

E se os dados fossem distribuídos de outra forma?

# Dados não lineares e o Teorema de Cover:

O Teorema de Cover nos diz que conjuntos de dados não lineares em espaços de alta dimensão são mais propensos a serem linearmente separáveis em comparação a espaços de menor dimensão, desde que o espaço não esteja excessivamente povoado.

![Dados não lineares](:SVM\TeoremadeCover.png){:data-align="center"}


Com base nisso, podemos aplicar uma transformação nos dados, como a função <img src="https://latex.codecogs.com/svg.image?(x^2,&space;2xy,y^2)" title="(x^2, 2xy,y^2)" />, para mapeá-los em um espaço de características de maior dimensão. Assim, torna-se viável aplicar o SVM linear nesse novo espaço de características.

![Espaço de características](:SVM\Espacodecaracterísticas.png){:data-align="center"}


No entanto, é importante observar que o cálculo da função de mapeamento para o espaço de características pode ser computacionalmente custoso devido à alta dimensionalidade.

# Truque do Kernel:

O truque do Kernel é uma técnica que nos permite evitar o mapeamento explícito dos dados para o espaço de características de alta dimensão.

Pelo Teorema de Mercer, a função Kernel recebe os pontos do espaço de entrada e calcula o produto escalar entre eles no espaço característica, desde que defina Kernel como matriz positivamente definida e que tenha autovalores maior do que zero.

Exemplificando, temos o kernel que define o mapeamento ϕ para uma dimensão de espaço mais alta.
Considere o seguinte: <img src="https://latex.codecogs.com/svg.image?\textbf{W}=&space;\left&space;(&space;x_1,y_1&space;\right&space;),&space;\textbf{Q}=(x_2,y_2&space;)" title="\textbf{W}= \left ( x_1,y_1 \right ), \textbf{Q}=(x_2,y_2 )" />. Assumimos a função do Kernel polinomial, com γ=1 , c=0 e d=2. Com estes parâmetros, é conhecido como Kernel quadrático.

<img src="https://latex.codecogs.com/png.image?\dpi{110}\begin{align*}&space;Kernel&space;Quadr&space;tico(\textbf{W},\textbf{Q})&space;&=&space;(\textbf{W}^T&space;\textbf{Q})^2&space;\\&space;&space;&=&space;(x_1&space;x_2&plus;y_1&space;y_2&space;)^2&space;&space;\\&space;&space;&=&space;x_1^2&space;x_2^2&plus;2x_1&space;x_2&space;y_1&space;y_2&plus;y_1^2&space;y_2^2&space;\\&space;&space;&=&space;(x_1^2&plus;&space;\sqrt{2x_1&space;y_1}&plus;y_1^2)^T&space;&space;(x_2^2&plus;\sqrt{2x_2&space;y_2}&space;&plus;y_2^2)&space;\\&space;&space;&=&space;&space;(\textbf{W})^T&space;&space;(\textbf{Q})&space;&space;\end{align*}&space;&space;&space;" title="\begin{align*} Kernel Quadr tico(\textbf{W},\textbf{Q}) &= (\textbf{W}^T \textbf{Q})^2 \\ &= (x_1 x_2+y_1 y_2 )^2 \\ &= x_1^2 x_2^2+2x_1 x_2 y_1 y_2+y_1^2 y_2^2 \\ &= (x_1^2+ \sqrt{2x_1 y_1}+y_1^2)^T (x_2^2+\sqrt{2x_2 y_2} +y_2^2) \\ &= (\textbf{W})^T (\textbf{Q}) \end{align*} " />

hkjh

O mapeamento dos pontos para uma dimensão de espaço mais alta, é dada pela função:
ϕ<img src="https://latex.codecogs.com/svg.image?&space;(Ponto)={x^2,&space;\sqrt{2xy},y^2}&space;" title=" (Ponto)={x^2, \sqrt{2xy},y^2} " />

Existem vários tipos de funções Kernel, como o polinomial, o radial e o de tangente hiperbólica. Essas funções definem o mapeamento para uma dimensão de espaço mais alta.

Polinomial -  <img src="https://latex.codecogs.com/svg.image?K(\textbf{x},\textbf{y})&space;=&space;(\gamma(\textbf{x}^T\textbf{y})&plus;c)^d" title="K(\textbf{x},\textbf{y}) = (\gamma(\textbf{x}^T\textbf{y})+c)^d" />

Radial -  <img src="https://latex.codecogs.com/svg.image?K(\textbf{x},\textbf{y})&space;=&space;e^{&space;-&space;\gamma&space;\left\|&space;x-y&space;\right\|^2}" title="K(\textbf{x},\textbf{y}) = e^{ - \gamma \left\| x-y \right\|^2}" />

Tangente hiperbólica -  <img src="https://latex.codecogs.com/svg.image?K(\textbf{x},\textbf{y})&space;=&space;tanh&space;\left&space;(&space;\gamma(\textbf{x}^T\textbf{y})&plus;c&space;\right&space;)" title="K(\textbf{x},\textbf{y}) = tanh \left ( \gamma(\textbf{x}^T\textbf{y})+c \right )" />

Mas se tivéssemos mais de 2 classes para classificar?


# Classificação SVM Multiclasse:

O SVM é originalmente projetado para classificação binária. No entanto, existem métodos que permitem a classificação de mais de duas classes.


![SVM Multiclasse](:SVM\SVMMulticlasse.png){:data-align="center"}

Alguns dos principais métodos são:

## Um-contra-todos:
Divide os dados em dois grupos, onde um grupo é formado por uma classe específica e o outro grupo contém todas as outras classes. Isso é repetido para cada classe.
## Um-contra-um:
Calcula a classificação entre uma classe e outra classe. Isso é repetido para todos os pares possíveis de classes.
## DAG-SVM:
É uma generalização da árvore de decisão, onde a árvore cresce por exclusão de classes, utilizando classificação binária.

# Referências:

* Shalev-Shwartz, Shai, and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.

* Faceli, Katti. Inteligência artificial: uma abordagem de aprendizado de máquina. Grupo Gen - LTC, 2011.

* Cover, Thomas M. “Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition.” IEEE Transactions on Electronic Computers, vol. EC-14, no. 3, June 1965, pp. 326–34. DOI.org (Crossref), [https://doi.org/10.1109/PGEC.1965.264137](https://doi.org/10.1109/PGEC.1965.264137). [[link]](https://isl.stanford.edu/~cover/papers/paper2.pdf)

* Chih-Wei Hsu and Chih-Jen Lin. “A Comparison of Methods for Multiclass Support Vector Machines.” IEEE Transactions on Neural Networks, vol. 13, no. 2, Mar. 2002, pp. 415–25. DOI.org (Crossref), [https://doi.org/10.1109/72.991427](https://doi.org/10.1109/72.991427).

* Platt, John and Cristianini, Nello and Shawe-Taylor, John. “Large Margin DAGs for Multiclass Classification”. Advances in Neural Information Processing Systems, MIT Press, vol.12, 1999. [[link]](https://proceedings.neurips.cc/paper_files/paper/1999/file/4abe17a1c80cbdd2aa241b70840879de-Paper.pdf)

* V.N. Vapnik and A.Ya. Chervonenkis. Theory of Pattern Recognition. Nauka, Moscow, 1974. (in Russian); German translation: Theorie der Zeichenerkennung, Akademie Verlag, Berlin, 1979.
