---
# multilingual page pair id, this must pair with translations of this page. (This name must be unique)
lng_pair: 4_SVM
title: "Desmistificando as Máquinas de Vetores de Suporte (SVM) para Classificação."

# post specific
# if not specified, .name will be used from _data/owner/[language].yml
#author: "Deborah Cholodoysky Barbedo Pereira"
# multiple category is not supported
category: Aprendizado de Máquina
# multiple tag entries are possible
tags: [SVM, Classificação, VC Dimension, Hiperplanos, Vetores de Suporte, Margens Flexíveis, Truque do Kernel, Classificação Multiclasse, Aprendizado de Máquina]
# thumbnail image for post
img: ":WoE_IV_Python_calculo.jpg"
# disable comments on this page
#comments_disable: true

# publish date
date: 2023-07-01 19:57:53 +0900

# seo
# if not specified, date will be used.
#meta_modify_date: 2021-08-10 11:32:53 +0900
# check the meta_common_description in _data/owner/[language].yml
meta_description: "Neste post, explore o funcionamento das Máquinas de Vetores de Suporte (SVM) para classificação. Descubra conceitos como VC Dimension, hiperplanos, vetores de suporte e o truque do kernel. Uma introdução acessível ao SVM para iniciantes no campo do Aprendizado de Máquina."
# optional
# please use the "image_viewer_on" below to enable image viewer for individual pages or posts (_posts/ or [language]/_posts folders).
# image viewer can be enabled or disabled for all posts using the "image_viewer_posts: true" setting in _data/conf/main.yml.
image_viewer_on: true
# please use the "image_lazy_loader_on" below to enable image lazy loader for individual pages or posts (_posts/ or [language]/_posts folders).
# image lazy loader can be enabled or disabled for all posts using the "image_lazy_loader_posts: true" setting in _data/conf/main.yml.
image_lazy_loader_on: true
# exclude from on site search
#on_site_search_exclude: true
# exclude from search engines
#search_engine_exclude: true
# to disable this page, simply set published: false or delete this file
#published: false
---

<!-- outline-start -->

Uma Exploração Inicial das SVM, Dimensão VC, Hiperplanos, Vetores de Suporte, Margens Flexíveis, Truque do Kernel e Classificação Multiclasse.

<!-- outline-end -->

A máquina de vetores de suporte, em inglês Support Vector Machine (SVM),  é um algoritmo versátil usado tanto para regressão quanto para classificação. Ele é especialmente útil quando lidamos com variáveis aleatórias numéricas. Neste post, vamos nos concentrar em explicar como o SVM funciona para problemas de classificação.

O objetivo aqui é tornar a explicação o mais acessível possível, evitando o uso excessivo de demonstrações matemáticas e teoremas que possam parecer assustadores. Quero apresentar o SVM de forma superficial para que você possa entender a lógica por trás dele. Assim, você estará preparado para se aprofundar no assunto, já com uma base sólida sobre o propósito do algoritmo, ou mesmo para uma compreensão básica que permita aplicá-lo rapidamente.

# Dimensão Vapnik-Chervonenkis (VC):

A dimensão Vapnik-Chervonenkis (VC) é uma medida que avalia a capacidade de um conjunto de funções serem aprendidas por um algoritmo de classificação binária estatística. Em termos mais simples, a dimensão VC de um conjunto de funções é o número máximo de pontos que podem ser divididos de formas diferentes por essas funções. Essa medida foi proposta por Vapnik e Chervonenkis em 1971, e é extremamente útil para compreender a flexibilidade de um conjunto de funções na tarefa de classificação.


Exemplo: Dimensão VC de um conjunto de funções lineares para classificação de 2 grupos em um plano, é igual a 3.


<div style="text-align: center;">
  <img src="image_2.png" alt="Image" />
</div>

Atualizou!

Se tivéssemos que realizar uma classificação binária com 4 pontos, não seria possível encontrar uma função linear para todas as combinações possíveis.

<p align="center">
  <img src="img_3.png" alt="Image" />
</p>


# Hiperplano:

Um hiperplano é um plano de dimensão k-1 em um espaço de dimensão k. Por exemplo, em um espaço bidimensional, um hiperplano é uma reta, enquanto em um espaço tridimensional, é um plano.
Um hiperplano é um plano (k-1)-dimensional na dimensão k.
Por exemplo:

<p align="center">
  <img src="img_4.png" alt="Image" />
</p>

Conectando com o tópico anterior, podemos dizer que a dimensão VC de um hiperplano para uma dimensão K, é K+1.

Existem vários hiperplanos possíveis para a classificação dos dados.
Conectando com o tópico anterior, a dimensão VC de um hiperplano em uma dimensão K é igual a K+1. Existem vários hiperplanos possíveis para classificar os dados, mas o SVM visa encontrar o hiperplano que possui a maior margem de separação entre as duas classes.

<p align="center">
  <img src="img_5.png" alt="Image" />
</p>


O objetivo do SVM é encontrar o hiperplano com maior margem de separação entre as duas classes.

<p align="center">
  <img src="img_6.png" alt="Image" />
</p>


# Vetores de suporte:
Os vetores de suporte, também conhecidos como pontos críticos, são os pontos de dados que auxiliam na determinação da margem do classificador linear. Eles são fundamentais para a identificação do hiperplano ideal.
Os vetores de suporte, também conhecidos como pontos críticos, que auxiliam a margem do classificador linear.

# Margens suaves:
Para evitar o sobreajuste, é possível adotar margens suaves, permitindo que alguns pontos de dados fiquem dentro da margem de separação. Isso flexibiliza a classificação e torna o modelo mais robusto.
Para evitar um sobreajuste, é possível adotar margens suaves.

![img_7.png](img_7.png)


<p align="center">
  <img src="img_7.png" alt="Image7" />
</p>

![img_8.png](img_8.png)

E se os dados fossem distribuídos de outra forma?

# Dados não lineares e o Teorema de Cover:
O Teorema de Cover nos diz que conjuntos de dados não lineares em espaços de alta dimensão são mais propensos a serem linearmente separáveis em comparação a espaços de menor dimensão, desde que o espaço não esteja excessivamente povoado.


“ UM CONJUNTO DE DADOS NÃO LINEARES EM UM ESPAÇO DE ALTA DIMENSÃO É MAIS SUCETÍVEL  A SER LINEARMENTE SEPARÁVEL DO QUE  EM UM ESPAÇO DE MENOR DIMENSÃO, NA CONDIÇÃO DE QUE O ESPAÇO NÃO SEJA EXCESSIVAMENTE POVOADO”. - TEOREMA DE COVER
Com o emprego de um procedimento motivado pelo teorema de Cover, e possível a
generalização de SVMs para problemas não lineares.
Então aumentamos a dimensão, aplicando a transformação (x^2,√2xy,y^2)
Com base nisso, podemos aplicar uma transformação nos dados, como a função (x^2,√2xy,y^2), para mapeá-los em um espaço de características de maior dimensão. Assim, torna-se viável aplicar o SVM linear nesse novo espaço de características.

Mapeando as amostras de treinamento do espaço original e transformando por meio
de uma função para um espaço de maior dimensão, chamado de espaço de características, se torna viável a aplicação do SVM linear.
No espaço característica, foi possível encontrar um plano que fosse capaz de dividir os dois grupos.
Mas a função para o espaço característica pode ter uma dimensão muito alta, e pode ser custosa computacionalmente para ser calculada.

. No entanto, é importante observar que o cálculo da função de mapeamento para o espaço de características pode ser computacionalmente custoso devido à alta dimensionalidade.

# Truque do KERNEL:
O truque do Kernel é uma técnica que nos permite evitar o mapeamento explícito dos dados para o espaço de características de alta dimensão. O Kernel recebe os pontos do espaço de entrada e calcula o produto escalar entre eles no espaço de características. O Kernel deve ser definido como uma matriz positivamente definida com autovalores maiores que zero.
Pelo Teorema de Mercer, a função Kernel recebe os pontos do espaço de entrada e calcula o produto escalar entre eles no espaço característica, desde que defina Kernel como matriz positivamente definida e que tenha autovalores maior do que zero.

Exemplificando, temos o kernel que define o mapeamento ϕ para uma dimensão de espaço mais alta.
Considere o seguinte: W=(x_1,y_1 ),  Q=(x_2,y_2 ). Assumimos a função do Kernel polinomial, com γ=1 , c=0 e d=2. Com estes parâmetros, é conhecido como Kernel quadrático.

Kernel Quadrático(W,Q)=(W^T Q)^2
=(x_1 x_2+y_1 y_2 )^2
= x_1^2 x_2^2+〖2x_1 x_2 y〗_1 y_2+y_1^2 y_2^2
= 〖〖(x〗_1^2+√(〖2x_1 y〗_1 )+y_1^2)〗^T  〖(x〗_2^2+√(〖2x_2 y〗_2 )+y_2^2)
=ϕ(W)^T ϕ(Q)
O mapeamento dos pontos para uma dimensão de espaço mais alta, é dada pela função:
ϕ(Ponto)={x^2, √2xy,y^2}

Existem vários tipos de funções Kernel, como o polinomial, o radial e o de tangente hiperbólica. Essas funções definem o mapeamento para uma dimensão de espaço mais alta.

Polinomial -

Radial -

Tangente hiperbólica -

Mas se tivéssemos mais de 2 classes para classificar?

# Classificação SVM Multiclasse:
SVM tem como objetivo a classificação binária. Para o classificar mais de duas classes existem alguns métodos que combinam os dados de treinamento de forma que seja possível aplicar o método de classificação binária.
O SVM é originalmente projetado para classificação binária. No entanto, existem métodos que permitem a classificação de mais de duas classes. Alguns dos principais métodos são:



Principais métodos :

## Um-contra-todos:
Divide os dados em dois grupos, onde um grupo é formado por uma classe específica e o outro grupo contém todas as outras classes. Isso é repetido para cada classe.
## Um-contra-um:
Calcula a classificação entre uma classe e outra classe. Isso é repetido para todos os pares possíveis de classes.
## DAG-SVM:
É uma generalização da árvore de decisão, onde a árvore cresce por exclusão de classes, utilizando classificação binária.
UM-CONTRA-TODOS
Divide em dois grupos, onde um é formado por apenas uma classe, e o outro formado pelas demais classes.
Isto é repetido para todas as classes.

UM-CONTRA-UM
Uma classe é calculada com outra classe.
Isto é repetido até que tenham sido realizados todos os pares possíveis.
DAG-SVM
Uma generalização da árvore de decisão.
Utilizando a classificação binária, a árvore vai crescendo por exclusão da classe.


# Referências:

* Shalev-Shwartz, Shai, and Shai Ben-David. Understanding Machine Learning: From Theory to Algorithms. Cambridge University Press, 2014.

* Faceli, Katti. Inteligência artificial: uma abordagem de aprendizado de máquina. Grupo Gen - LTC, 2011.

* Cover, Thomas M. “Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition.” IEEE Transactions on Electronic Computers, vol. EC-14, no. 3, June 1965, pp. 326–34. DOI.org (Crossref), https://doi.org/10.1109/PGEC.1965.264137. [link](https://isl.stanford.edu/~cover/papers/paper2.pdf)

* Chih-Wei Hsu and Chih-Jen Lin. “A Comparison of Methods for Multiclass Support Vector Machines.” IEEE Transactions on Neural Networks, vol. 13, no. 2, Mar. 2002, pp. 415–25. DOI.org (Crossref), https://doi.org/10.1109/72.991427.

* Platt, John and Cristianini, Nello and Shawe-Taylor, John. “Large Margin DAGs for Multiclass Classification”. Advances in Neural Information Processing Systems, MIT Press, vol.12, 1999. [link](https://proceedings.neurips.cc/paper_files/paper/1999/file/4abe17a1c80cbdd2aa241b70840879de-Paper.pdf)

* V.N. Vapnik and A.Ya. Chervonenkis. Theory of Pattern Recognition. Nauka, Moscow, 1974. (in Russian); German translation: Theorie der Zeichenerkennung, Akademie Verlag, Berlin, 1979.
